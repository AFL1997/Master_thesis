{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Packages used in the script\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas.io.json import json_normalize\n",
    "import io\n",
    "import gzip\n",
    "import spacy\n",
    "import re\n",
    "from contractions import contractions_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_spo = pd.read_csv (r'Path where the CSV file is stored\\df_sports.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.layers import SpatialDropout1D\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras import optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = 1800\n",
    "maxlen = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_tokenizer = Tokenizer()\n",
    "\n",
    "X = df_spo['sentences']\n",
    "word_tokenizer.fit_on_texts(X)\n",
    "\n",
    "X_encoded = word_tokenizer.texts_to_sequences(X) \n",
    "\n",
    "X_padded = pad_sequences(X_encoded, maxlen=maxlen, padding='post', truncating='post')\n",
    "\n",
    "X = X_padded.astype('int')\n",
    "X = np.array(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'I_neg', 'O', 'I_pos', 'B_neg', 'B_pos'}\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "tags = set(itertools.chain.from_iterable(df_spo.IOB))\n",
    "print(tags)\n",
    "n_tags = len(tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag2idx = {t: i for i, t in enumerate(tags)}\n",
    "tag2idx['O']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = [[tag2idx[w] for w in s] for s in df_spo['IOB']]\n",
    "y = pad_sequences(y, maxlen=maxlen, padding='post', truncating='post', value = tag2idx['O'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = to_categorical(y, dtype = 'int', num_classes= 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_size  = 300  \n",
    "vocabulary_size = len(word_tokenizer.word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_weights = np.zeros((vocabulary_size, embedding_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total number of words\n",
    "word2id = word_tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split in train and test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4000, 50) (4000, 50, 5)\n",
      "(1000, 50) (1000, 50, 5)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape, y_train.shape)\n",
    "print(X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = optimizers.Adam(learning_rate=0.01, decay=1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_14\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_14 (Embedding)     (None, 50, 300)           2224800   \n",
      "_________________________________________________________________\n",
      "lstm_14 (LSTM)               (None, 50, 64)            93440     \n",
      "_________________________________________________________________\n",
      "time_distributed_12 (TimeDis (None, 50, 5)             325       \n",
      "=================================================================\n",
      "Total params: 2,318,565\n",
      "Trainable params: 2,318,565\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "32/32 [==============================] - 4s 77ms/step - loss: 0.2245 - accuracy: 0.9455\n",
      "Epoch 2/10\n",
      "32/32 [==============================] - 2s 76ms/step - loss: 0.0874 - accuracy: 0.9750\n",
      "Epoch 3/10\n",
      "32/32 [==============================] - 3s 83ms/step - loss: 0.0761 - accuracy: 0.9770\n",
      "Epoch 4/10\n",
      "32/32 [==============================] - 3s 80ms/step - loss: 0.0688 - accuracy: 0.9784\n",
      "Epoch 5/10\n",
      "32/32 [==============================] - 2s 77ms/step - loss: 0.0626 - accuracy: 0.9794\n",
      "Epoch 6/10\n",
      "32/32 [==============================] - 2s 77ms/step - loss: 0.0575 - accuracy: 0.9805\n",
      "Epoch 7/10\n",
      "32/32 [==============================] - 2s 77ms/step - loss: 0.0525 - accuracy: 0.9817\n",
      "Epoch 8/10\n",
      "32/32 [==============================] - 2s 78ms/step - loss: 0.0486 - accuracy: 0.9830\n",
      "Epoch 9/10\n",
      "32/32 [==============================] - 2s 76ms/step - loss: 0.0449 - accuracy: 0.9838\n",
      "Epoch 10/10\n",
      "32/32 [==============================] - 2s 77ms/step - loss: 0.0421 - accuracy: 0.9845\n"
     ]
    }
   ],
   "source": [
    "lstm_model = keras.Sequential()\n",
    "lstm_model.add(tf.keras.layers.Embedding(vocabulary_size, embedding_size, input_length = maxlen)) #The embedding layer\n",
    "lstm_model.add(tf.keras.layers.LSTM(64, dropout=0.6, return_sequences=True)) #Our LSTM layer\n",
    "lstm_model.add(tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(5, activation='softmax')))\n",
    "lstm_model.summary()\n",
    "\n",
    "lstm_model.compile(opt, \"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "history = lstm_model.fit(X_train, y_train, batch_size = 128, epochs=10) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 1s 8ms/step - loss: 0.1023 - accuracy: 0.9765\n",
      "Loss: 0.1022549420595169,\n",
      "Accuracy: 0.9765400290489197\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = lstm_model.evaluate(X_test, y_test, verbose = 1)\n",
    "print('Loss: {0},\\nAccuracy: {1}'.format(loss, accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict\n",
    "y_pred_lstm = lstm_model.predict(X_test)\n",
    "\n",
    "# Make hard classes - https://stackoverflow.com/questions/43672047/convert-probability-vector-into-target-vector-in-python\n",
    "y_pred_lstm[y_pred_lstm > 0.5] = 1\n",
    "y_pred_lstm[y_pred_lstm <= 0.5] = 0\n",
    "\n",
    "y_pred_lstm = np.array(y_pred_lstm.astype('int'))\n",
    "\n",
    "pred_lstm = np.array(tf.argmax(y_pred_lstm, axis = -1))\n",
    "y_true = np.array(tf.argmax(y_test, axis = -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 48751,   1249],\n",
       "        [  1119, 198881]],\n",
       "\n",
       "       [[198881,   1119],\n",
       "        [  1249,  48751]]])"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Confusion matrix LSTM model - https://scikit-learn.org/stable/modules/generated/sklearn.metrics.multilabel_confusion_matrix.html\n",
    "from sklearn.metrics import multilabel_confusion_matrix\n",
    "multilabel_confusion_matrix(y_test.flatten(), y_pred_lstm.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.01      0.05      0.01        21\n",
      "           1       0.98      0.99      0.99     48780\n",
      "           2       0.38      0.10      0.16       109\n",
      "           3       0.63      0.15      0.24       162\n",
      "           4       0.54      0.32      0.40       928\n",
      "\n",
      "    accuracy                           0.98     50000\n",
      "   macro avg       0.51      0.32      0.36     50000\n",
      "weighted avg       0.97      0.98      0.97     50000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Classification report LSTM model\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_true.flatten(), pred_lstm.flatten()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.97756165\n"
     ]
    }
   ],
   "source": [
    "# Precision LSTM model - https://www.tensorflow.org/api_docs/python/tf/keras/metrics/Precision\n",
    "m = tf.keras.metrics.Precision()\n",
    "m.update_state(y_test, y_pred_lstm)\n",
    "precision = m.result().numpy()\n",
    "print(precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.97502\n"
     ]
    }
   ],
   "source": [
    "# Recall LSTM model - https://www.tensorflow.org/api_docs/python/tf/keras/metrics/Recall\n",
    "m = tf.keras.metrics.Recall()\n",
    "m.update_state(y_test, y_pred_lstm)\n",
    "recall = m.result().numpy()\n",
    "print(recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.976289180058759\n"
     ]
    }
   ],
   "source": [
    "# F1-score LSTM model - https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html\n",
    "F1 = 2 * (precision * recall) / (precision + recall)\n",
    "print(F1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_15\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_15 (Embedding)     (None, 50, 300)           2224800   \n",
      "_________________________________________________________________\n",
      "bidirectional_6 (Bidirection (None, 50, 128)           186880    \n",
      "_________________________________________________________________\n",
      "time_distributed_13 (TimeDis (None, 50, 5)             645       \n",
      "=================================================================\n",
      "Total params: 2,412,325\n",
      "Trainable params: 2,412,325\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "32/32 [==============================] - 6s 116ms/step - loss: 0.1838 - accuracy: 0.9455\n",
      "Epoch 2/10\n",
      "32/32 [==============================] - 4s 118ms/step - loss: 0.0844 - accuracy: 0.9751\n",
      "Epoch 3/10\n",
      "32/32 [==============================] - 4s 116ms/step - loss: 0.0729 - accuracy: 0.9770\n",
      "Epoch 4/10\n",
      "32/32 [==============================] - 4s 120ms/step - loss: 0.0631 - accuracy: 0.9793\n",
      "Epoch 5/10\n",
      "32/32 [==============================] - 4s 118ms/step - loss: 0.0551 - accuracy: 0.9819\n",
      "Epoch 6/10\n",
      "32/32 [==============================] - 4s 117ms/step - loss: 0.0486 - accuracy: 0.9838\n",
      "Epoch 7/10\n",
      "32/32 [==============================] - 4s 119ms/step - loss: 0.0434 - accuracy: 0.9850\n",
      "Epoch 8/10\n",
      "32/32 [==============================] - 4s 120ms/step - loss: 0.0389 - accuracy: 0.9862\n",
      "Epoch 9/10\n",
      "32/32 [==============================] - 4s 119ms/step - loss: 0.0356 - accuracy: 0.9874\n",
      "Epoch 10/10\n",
      "32/32 [==============================] - 4s 120ms/step - loss: 0.0318 - accuracy: 0.9886\n"
     ]
    }
   ],
   "source": [
    "#Bidirectional LSTM https://github.com/sergiovirahonda/TweetsSentimentAnalysis/blob/main/TweetsSentimentPredictions.ipynb\n",
    "bilstm_model = keras.Sequential()\n",
    "bilstm_model.add(tf.keras.layers.Embedding(vocabulary_size, embedding_size, input_length = maxlen))\n",
    "bilstm_model.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64,dropout=0.6, return_sequences=True)))\n",
    "bilstm_model.add(tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(5, activation='softmax')))\n",
    "bilstm_model.summary()\n",
    "\n",
    "bilstm_model.compile(optimizer=opt,loss='categorical_crossentropy', metrics=[\"accuracy\"])\n",
    "\n",
    "history = bilstm_model.fit(X_train, y_train, batch_size = 128, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 1s 14ms/step - loss: 0.0997 - accuracy: 0.9751\n",
      "Loss: 0.09966423362493515,\n",
      "Accuracy: 0.9751399755477905\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = bilstm_model.evaluate(X_test, y_test, verbose = 1)\n",
    "print(\"Loss: {0},\\nAccuracy: {1}\".format(loss, accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict\n",
    "y_pred_bilstm = bilstm_model.predict(X_test)\n",
    "\n",
    "# Make hard classes\n",
    "y_pred_bilstm[y_pred_bilstm > 0.5] = 1\n",
    "y_pred_bilstm[y_pred_bilstm <= 0.5] = 0\n",
    "\n",
    "y_pred_bilstm = np.array(y_pred_bilstm.astype('int'))\n",
    "\n",
    "pred_bilstm = np.array(tf.argmax(y_pred_bilstm, axis = -1))\n",
    "y_true = np.array(tf.argmax(y_test, axis = -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 48704,   1296],\n",
       "        [  1181, 198819]],\n",
       "\n",
       "       [[198819,   1181],\n",
       "        [  1296,  48704]]])"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Confusion matrix LSTM model - https://scikit-learn.org/stable/modules/generated/sklearn.metrics.multilabel_confusion_matrix.html\n",
    "from sklearn.metrics import multilabel_confusion_matrix\n",
    "multilabel_confusion_matrix(y_test.flatten(), y_pred_bilstm.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.01      0.05      0.01        21\n",
      "           1       0.99      0.99      0.99     48780\n",
      "           2       0.35      0.17      0.22       109\n",
      "           3       0.47      0.30      0.36       162\n",
      "           4       0.50      0.50      0.50       928\n",
      "\n",
      "    accuracy                           0.97     50000\n",
      "   macro avg       0.46      0.40      0.42     50000\n",
      "weighted avg       0.97      0.97      0.97     50000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Classification report LSTM model\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_true.flatten(), pred_bilstm.flatten()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9763256\n"
     ]
    }
   ],
   "source": [
    "# Precision LSTM model - https://www.tensorflow.org/api_docs/python/tf/keras/metrics/Precision\n",
    "m = tf.keras.metrics.Precision()\n",
    "m.update_state(y_test, y_pred_bilstm)\n",
    "precision = m.result().numpy()\n",
    "print(precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.97408\n"
     ]
    }
   ],
   "source": [
    "# Recall LSTM model - https://www.tensorflow.org/api_docs/python/tf/keras/metrics/Recall\n",
    "m = tf.keras.metrics.Recall()\n",
    "m.update_state(y_test, y_pred_bilstm)\n",
    "recall = m.result().numpy()\n",
    "print(recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9752014951481518\n"
     ]
    }
   ],
   "source": [
    "# F1-score LSTM model - https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html\n",
    "F1 = 2 * (precision * recall) / (precision + recall)\n",
    "print(F1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add an attention-layer\n",
    "from tensorflow.keras.layers import Dense, Lambda, Dot, Activation, Concatenate\n",
    "from tensorflow.keras.layers import Layer\n",
    "\n",
    "class Attention(Layer):\n",
    "\n",
    "    def __init__(self, units=128, **kwargs):\n",
    "        self.units = units\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    def __call__(self, inputs):\n",
    "        \"\"\"\n",
    "        Many-to-one attention mechanism for Keras.\n",
    "        @param inputs: 3D tensor with shape (batch_size, time_steps, input_dim).\n",
    "        @return: 2D tensor with shape (batch_size, 128)\n",
    "        @author: felixhao28, philipperemy.\n",
    "        \"\"\"\n",
    "        hidden_states = inputs\n",
    "        hidden_size = int(hidden_states.shape[2])\n",
    "        # Inside dense layer\n",
    "        #              hidden_states            dot               W            =>           score_first_part\n",
    "        # (batch_size, time_steps, hidden_size) dot (hidden_size, hidden_size) => (batch_size, time_steps, hidden_size)\n",
    "        # W is the trainable weight matrix of attention Luong's multiplicative style score\n",
    "        score_first_part = Dense(hidden_size, use_bias=False, name='attention_score_vec')(hidden_states)\n",
    "        #            score_first_part           dot        last_hidden_state     => attention_weights\n",
    "        # (batch_size, time_steps, hidden_size) dot   (batch_size, hidden_size)  => (batch_size, time_steps)\n",
    "        h_t = Lambda(lambda x: x[:, -1, :], output_shape=(hidden_size,), name='last_hidden_state')(hidden_states)\n",
    "        score = Dot(axes=[1, 2], name='attention_score')([h_t, score_first_part])\n",
    "        attention_weights = Activation('softmax', name='attention_weight')(score)\n",
    "        # (batch_size, time_steps, hidden_size) dot (batch_size, time_steps) => (batch_size, hidden_size)\n",
    "        context_vector = Dot(axes=[1, 1], name='context_vector')([hidden_states, attention_weights])\n",
    "        pre_activation = Concatenate(name='attention_output')([context_vector, h_t])\n",
    "        attention_vector = Dense(self.units, use_bias=False, activation='tanh', name='attention_vector')(pre_activation)\n",
    "        return attention_vector\n",
    "\n",
    "    def get_config(self):\n",
    "        return {'units': self.units}\n",
    "\n",
    "    @classmethod\n",
    "    def from_config(cls, config):\n",
    "        return cls(**config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_16\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_16 (Embedding)     (None, 50, 300)           2224800   \n",
      "_________________________________________________________________\n",
      "bidirectional_7 (Bidirection (None, 50, 128)           186880    \n",
      "_________________________________________________________________\n",
      "layer_2 (Layer)              (None, 50, 128)           0         \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 50, 5)             645       \n",
      "=================================================================\n",
      "Total params: 2,412,325\n",
      "Trainable params: 2,412,325\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "32/32 [==============================] - 7s 103ms/step - loss: 0.1865 - accuracy: 0.9447\n",
      "Epoch 2/10\n",
      "32/32 [==============================] - 3s 104ms/step - loss: 0.0838 - accuracy: 0.9751\n",
      "Epoch 3/10\n",
      "32/32 [==============================] - 3s 104ms/step - loss: 0.0754 - accuracy: 0.9763\n",
      "Epoch 4/10\n",
      "32/32 [==============================] - 4s 109ms/step - loss: 0.0695 - accuracy: 0.9780\n",
      "Epoch 5/10\n",
      "32/32 [==============================] - 4s 110ms/step - loss: 0.0633 - accuracy: 0.9800\n",
      "Epoch 6/10\n",
      "32/32 [==============================] - 4s 109ms/step - loss: 0.0562 - accuracy: 0.9821\n",
      "Epoch 7/10\n",
      "32/32 [==============================] - 4s 109ms/step - loss: 0.0504 - accuracy: 0.9838\n",
      "Epoch 8/10\n",
      "32/32 [==============================] - 3s 107ms/step - loss: 0.0451 - accuracy: 0.9855\n",
      "Epoch 9/10\n",
      "32/32 [==============================] - 3s 109ms/step - loss: 0.0411 - accuracy: 0.9868\n",
      "Epoch 10/10\n",
      "32/32 [==============================] - 4s 109ms/step - loss: 0.0360 - accuracy: 0.9882\n"
     ]
    }
   ],
   "source": [
    "# Attention-based LSTM model\n",
    "import os\n",
    "os.environ['TF_KERAS'] = '1'\n",
    "from keras_self_attention import SeqSelfAttention\n",
    "\n",
    "attentionlstm_model = keras.Sequential()\n",
    "attentionlstm_model.add(tf.keras.layers.Embedding(vocabulary_size, embedding_size, input_length = maxlen))# weights = [embedding_weights], trainable = True))\n",
    "attentionlstm_model.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64, return_sequences=True)))\n",
    "\n",
    "attentionlstm_model.add(Layer(SeqSelfAttention(attention_activation='sigmoid')))\n",
    "\n",
    "attentionlstm_model.add(tf.keras.layers.Dense(5, 'softmax'))\n",
    "\n",
    "attentionlstm_model.summary()\n",
    "attentionlstm_model.compile(optimizer=opt,loss='categorical_crossentropy', metrics=[\"accuracy\"])\n",
    "\n",
    "history = attentionlstm_model.fit(X_train, y_train, batch_size = 128, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 1s 14ms/step - loss: 0.1025 - accuracy: 0.9765\n",
      "Loss: 0.10252467542886734,\n",
      "Accuracy: 0.9764800071716309\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = attentionlstm_model.evaluate(X_test, y_test, verbose = 1)\n",
    "print(\"Loss: {0},\\nAccuracy: {1}\".format(loss, accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict\n",
    "y_pred_attentionlstm = attentionlstm_model.predict(X_test)\n",
    "\n",
    "# Make hard classes\n",
    "y_pred_attentionlstm[y_pred_attentionlstm > 0.5] = 1\n",
    "y_pred_attentionlstm[y_pred_attentionlstm <= 0.5] = 0\n",
    "\n",
    "y_pred_attentionlstm = np.array(y_pred_attentionlstm.astype('int'))\n",
    "\n",
    "pred_attentionlstm = np.array(tf.argmax(y_pred_attentionlstm, axis = -1))\n",
    "y_true = np.array(tf.argmax(y_test, axis = -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 48760,   1240],\n",
       "        [  1109, 198891]],\n",
       "\n",
       "       [[198891,   1109],\n",
       "        [  1240,  48760]]])"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Confusion matrix LSTM model - https://scikit-learn.org/stable/modules/generated/sklearn.metrics.multilabel_confusion_matrix.html\n",
    "from sklearn.metrics import multilabel_confusion_matrix\n",
    "multilabel_confusion_matrix(y_test.flatten(), y_pred_attentionlstm.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.02      0.10      0.03        21\n",
      "           1       0.98      0.99      0.99     48780\n",
      "           2       0.28      0.12      0.17       109\n",
      "           3       0.56      0.15      0.23       162\n",
      "           4       0.55      0.39      0.45       928\n",
      "\n",
      "    accuracy                           0.98     50000\n",
      "   macro avg       0.48      0.35      0.37     50000\n",
      "weighted avg       0.97      0.98      0.97     50000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Classification report LSTM model\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_true.flatten(), pred_attentionlstm.flatten()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.97776175\n"
     ]
    }
   ],
   "source": [
    "# Precision LSTM model - https://www.tensorflow.org/api_docs/python/tf/keras/metrics/Precision\n",
    "m = tf.keras.metrics.Precision()\n",
    "m.update_state(y_test, y_pred_attentionlstm)\n",
    "precision = m.result().numpy()\n",
    "print(precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9752\n"
     ]
    }
   ],
   "source": [
    "# Recall LSTM model - https://www.tensorflow.org/api_docs/python/tf/keras/metrics/Recall\n",
    "m = tf.keras.metrics.Recall()\n",
    "m.update_state(y_test, y_pred_attentionlstm)\n",
    "recall = m.result().numpy()\n",
    "print(recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9764792344086389\n"
     ]
    }
   ],
   "source": [
    "# F1-score LSTM model - https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html\n",
    "F1 = 2 * (precision * recall) / (precision + recall)\n",
    "print(F1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
