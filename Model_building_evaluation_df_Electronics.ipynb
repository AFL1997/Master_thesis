{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Packages used in the script\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas.io.json import json_normalize\n",
    "import io\n",
    "import gzip\n",
    "import spacy\n",
    "import re\n",
    "from contractions import contractions_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_elec = pd.read_csv (r'Path where the CSV file is stored\\df_electronics.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 669,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.layers import SpatialDropout1D\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras import optimizers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 670,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = 1800\n",
    "maxlen = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 671,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_tokenizer = Tokenizer()\n",
    "\n",
    "X = df_elec['sentences']\n",
    "word_tokenizer.fit_on_texts(X)\n",
    "\n",
    "X_encoded = word_tokenizer.texts_to_sequences(X) \n",
    "\n",
    "X_padded = pad_sequences(X_encoded, maxlen=maxlen, padding='post', truncating='post')\n",
    "\n",
    "X = X_padded.astype('int')\n",
    "X = np.array(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 674,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'B_pos', 'B_neg', 'I_pos', 'O', 'I_neg'}\n"
     ]
    }
   ],
   "source": [
    "# We have in total 5 tags\n",
    "import itertools\n",
    "tags = set(itertools.chain.from_iterable(df_elec.IOB))\n",
    "print(tags)\n",
    "n_tags = len(tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 675,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 675,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag2idx = {t: i for i, t in enumerate(tags)}\n",
    "tag2idx['O']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 676,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = [[tag2idx[w] for w in s] for s in df_elec['IOB']]\n",
    "\n",
    "\n",
    "y = pad_sequences(y, maxlen=maxlen, padding='post', truncating='post', value = tag2idx['O'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 677,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = to_categorical(y, dtype = 'int', num_classes= 5)\n",
    "                   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 679,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_size  = 300  \n",
    "vocabulary_size = len(word_tokenizer.word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 680,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_weights = np.zeros((vocabulary_size, embedding_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 681,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total number of words\n",
    "word2id = word_tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 682,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  17,  158,    7, 1752,    4,   87,  184,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0])"
      ]
     },
     "execution_count": 682,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 684,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000, 50)\n",
      "(5000, 50, 5)\n"
     ]
    }
   ],
   "source": [
    "print(X_padded.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 685,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split in train and test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 686,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4000, 50) (4000, 50, 5)\n",
      "(1000, 50) (1000, 50, 5)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape, y_train.shape)\n",
    "print(X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 687,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4000, 1000)"
      ]
     },
     "execution_count": 687,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_train), len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 689,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = optimizers.Adam(learning_rate=0.01, decay=1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 690,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 691,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_56\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_56 (Embedding)     (None, 50, 300)           2563500   \n",
      "_________________________________________________________________\n",
      "lstm_56 (LSTM)               (None, 50, 64)            93440     \n",
      "_________________________________________________________________\n",
      "time_distributed_45 (TimeDis (None, 50, 5)             325       \n",
      "=================================================================\n",
      "Total params: 2,657,265\n",
      "Trainable params: 2,657,265\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "32/32 [==============================] - 4s 82ms/step - loss: 0.2311 - accuracy: 0.9454\n",
      "Epoch 2/10\n",
      "32/32 [==============================] - 3s 81ms/step - loss: 0.1007 - accuracy: 0.9698\n",
      "Epoch 3/10\n",
      "32/32 [==============================] - 3s 80ms/step - loss: 0.0894 - accuracy: 0.9717\n",
      "Epoch 4/10\n",
      "32/32 [==============================] - 3s 81ms/step - loss: 0.0794 - accuracy: 0.9739\n",
      "Epoch 5/10\n",
      "32/32 [==============================] - 3s 81ms/step - loss: 0.0709 - accuracy: 0.9755\n",
      "Epoch 6/10\n",
      "32/32 [==============================] - 3s 81ms/step - loss: 0.0632 - accuracy: 0.9776\n",
      "Epoch 7/10\n",
      "32/32 [==============================] - 3s 83ms/step - loss: 0.0586 - accuracy: 0.9786\n",
      "Epoch 8/10\n",
      "32/32 [==============================] - 3s 83ms/step - loss: 0.0549 - accuracy: 0.9798\n",
      "Epoch 9/10\n",
      "32/32 [==============================] - 3s 82ms/step - loss: 0.0509 - accuracy: 0.9808\n",
      "Epoch 10/10\n",
      "32/32 [==============================] - 3s 81ms/step - loss: 0.0472 - accuracy: 0.9818\n"
     ]
    }
   ],
   "source": [
    "lstm_model = keras.Sequential()\n",
    "lstm_model.add(tf.keras.layers.Embedding(vocabulary_size, embedding_size, input_length = maxlen)) #The embedding layer\n",
    "lstm_model.add(tf.keras.layers.LSTM(64, dropout=0.6, return_sequences=True)) #Our LSTM layer\n",
    "lstm_model.add(tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(5, activation='softmax')))\n",
    "lstm_model.summary()\n",
    "\n",
    "lstm_model.compile(opt, \"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "history = lstm_model.fit(X_train, y_train, batch_size = 128, epochs=10) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 692,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 1s 8ms/step - loss: 0.1232 - accuracy: 0.9667\n",
      "Loss: 0.12321771681308746,\n",
      "Accuracy: 0.9667199850082397\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = lstm_model.evaluate(X_test, y_test, verbose = 1)\n",
    "print('Loss: {0},\\nAccuracy: {1}'.format(loss, accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 693,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict\n",
    "y_pred_lstm = lstm_model.predict(X_test)\n",
    "\n",
    "# Make hard classes \n",
    "y_pred_lstm[y_pred_lstm > 0.5] = 1\n",
    "y_pred_lstm[y_pred_lstm <= 0.5] = 0\n",
    "\n",
    "y_pred_lstm = np.array(y_pred_lstm.astype('int'))\n",
    "\n",
    "pred_lstm = np.array(tf.argmax(y_pred_lstm, axis = -1))\n",
    "y_true = np.array(tf.argmax(y_test, axis = -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 694,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 48136,   1864],\n",
       "        [  1464, 198536]],\n",
       "\n",
       "       [[198536,   1464],\n",
       "        [  1864,  48136]]])"
      ]
     },
     "execution_count": 694,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Confusion matrix LSTM model - https://scikit-learn.org/stable/modules/generated/sklearn.metrics.multilabel_confusion_matrix.html\n",
    "from sklearn.metrics import multilabel_confusion_matrix\n",
    "multilabel_confusion_matrix(y_test.flatten(), y_pred_lstm.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 695,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.40      0.44      0.42      1150\n",
      "           1       0.41      0.31      0.36       194\n",
      "           2       0.21      0.06      0.10       175\n",
      "           3       0.98      0.98      0.98     48449\n",
      "           4       1.00      0.03      0.06        32\n",
      "\n",
      "    accuracy                           0.96     50000\n",
      "   macro avg       0.60      0.37      0.38     50000\n",
      "weighted avg       0.96      0.96      0.96     50000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Classification report LSTM model\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_true.flatten(), pred_lstm.flatten()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 696,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9704839\n"
     ]
    }
   ],
   "source": [
    "# Precision LSTM model - https://www.tensorflow.org/api_docs/python/tf/keras/metrics/Precision\n",
    "m = tf.keras.metrics.Precision()\n",
    "m.update_state(y_test, y_pred_lstm)\n",
    "precision = m.result().numpy()\n",
    "print(precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 697,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.96272\n"
     ]
    }
   ],
   "source": [
    "# Recall LSTM model - https://www.tensorflow.org/api_docs/python/tf/keras/metrics/Recall\n",
    "m = tf.keras.metrics.Recall()\n",
    "m.update_state(y_test, y_pred_lstm)\n",
    "recall = m.result().numpy()\n",
    "print(recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 698,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9665863183384731\n"
     ]
    }
   ],
   "source": [
    "# F1-score LSTM model - https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html\n",
    "F1 = 2 * (precision * recall) / (precision + recall)\n",
    "print(F1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 699,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_57\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_57 (Embedding)     (None, 50, 300)           2563500   \n",
      "_________________________________________________________________\n",
      "bidirectional_21 (Bidirectio (None, 50, 128)           186880    \n",
      "_________________________________________________________________\n",
      "time_distributed_46 (TimeDis (None, 50, 5)             645       \n",
      "=================================================================\n",
      "Total params: 2,751,025\n",
      "Trainable params: 2,751,025\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "32/32 [==============================] - 6s 124ms/step - loss: 0.2052 - accuracy: 0.9398\n",
      "Epoch 2/10\n",
      "32/32 [==============================] - 4s 122ms/step - loss: 0.0995 - accuracy: 0.9702\n",
      "Epoch 3/10\n",
      "32/32 [==============================] - 4s 120ms/step - loss: 0.0887 - accuracy: 0.9719\n",
      "Epoch 4/10\n",
      "32/32 [==============================] - 4s 125ms/step - loss: 0.0807 - accuracy: 0.9744\n",
      "Epoch 5/10\n",
      "32/32 [==============================] - 4s 121ms/step - loss: 0.0719 - accuracy: 0.9761\n",
      "Epoch 6/10\n",
      "32/32 [==============================] - 4s 122ms/step - loss: 0.0654 - accuracy: 0.9776\n",
      "Epoch 7/10\n",
      "32/32 [==============================] - 4s 124ms/step - loss: 0.0589 - accuracy: 0.9795\n",
      "Epoch 8/10\n",
      "32/32 [==============================] - 4s 123ms/step - loss: 0.0537 - accuracy: 0.9812\n",
      "Epoch 9/10\n",
      "32/32 [==============================] - 4s 124ms/step - loss: 0.0489 - accuracy: 0.9825\n",
      "Epoch 10/10\n",
      "32/32 [==============================] - 4s 126ms/step - loss: 0.0451 - accuracy: 0.9836\n"
     ]
    }
   ],
   "source": [
    "#Bidirectional LSTM https://github.com/sergiovirahonda/TweetsSentimentAnalysis/blob/main/TweetsSentimentPredictions.ipynb\n",
    "bilstm_model = keras.Sequential()\n",
    "bilstm_model.add(tf.keras.layers.Embedding(vocabulary_size, embedding_size, input_length = maxlen))\n",
    "bilstm_model.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64,dropout=0.6, return_sequences=True)))\n",
    "bilstm_model.add(tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(5, activation='softmax')))\n",
    "bilstm_model.summary()\n",
    "\n",
    "\n",
    "\n",
    "bilstm_model.compile(optimizer=opt,loss='categorical_crossentropy', metrics=[\"accuracy\"])\n",
    "\n",
    "history = bilstm_model.fit(X_train, y_train, batch_size = 128, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 700,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 1s 14ms/step - loss: 0.1201 - accuracy: 0.9656\n",
      "Loss: 0.12014035880565643,\n",
      "Accuracy: 0.9656400084495544\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = bilstm_model.evaluate(X_test, y_test, verbose = 1)\n",
    "print(\"Loss: {0},\\nAccuracy: {1}\".format(loss, accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 701,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict\n",
    "y_pred_bilstm = bilstm_model.predict(X_test)\n",
    "\n",
    "# Make hard classes\n",
    "y_pred_bilstm[y_pred_bilstm > 0.5] = 1\n",
    "y_pred_bilstm[y_pred_bilstm <= 0.5] = 0\n",
    "\n",
    "y_pred_bilstm = np.array(y_pred_bilstm.astype('int'))\n",
    "\n",
    "pred_bilstm = np.array(tf.argmax(y_pred_bilstm, axis = -1))\n",
    "y_true = np.array(tf.argmax(y_test, axis = -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 702,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 48177,   1823],\n",
       "        [  1593, 198407]],\n",
       "\n",
       "       [[198407,   1593],\n",
       "        [  1823,  48177]]])"
      ]
     },
     "execution_count": 702,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Confusion matrix LSTM model - https://scikit-learn.org/stable/modules/generated/sklearn.metrics.multilabel_confusion_matrix.html\n",
    "from sklearn.metrics import multilabel_confusion_matrix\n",
    "multilabel_confusion_matrix(y_test.flatten(), y_pred_bilstm.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 703,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.40      0.45      0.42      1150\n",
      "           1       0.48      0.26      0.33       194\n",
      "           2       0.18      0.07      0.11       175\n",
      "           3       0.98      0.98      0.98     48449\n",
      "           4       0.50      0.03      0.06        32\n",
      "\n",
      "    accuracy                           0.96     50000\n",
      "   macro avg       0.51      0.36      0.38     50000\n",
      "weighted avg       0.96      0.96      0.96     50000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Classification report LSTM model\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_true.flatten(), pred_bilstm.flatten()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 704,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9679928\n"
     ]
    }
   ],
   "source": [
    "# Precision LSTM model - https://www.tensorflow.org/api_docs/python/tf/keras/metrics/Precision\n",
    "m = tf.keras.metrics.Precision()\n",
    "m.update_state(y_test, y_pred_bilstm)\n",
    "precision = m.result().numpy()\n",
    "print(precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 705,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.96354\n"
     ]
    }
   ],
   "source": [
    "# Recall LSTM model - https://www.tensorflow.org/api_docs/python/tf/keras/metrics/Recall\n",
    "m = tf.keras.metrics.Recall()\n",
    "m.update_state(y_test, y_pred_bilstm)\n",
    "recall = m.result().numpy()\n",
    "print(recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 706,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9657612551651337\n"
     ]
    }
   ],
   "source": [
    "# F1-score LSTM model - https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html\n",
    "F1 = 2 * (precision * recall) / (precision + recall)\n",
    "print(F1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 707,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add an attention-layer\n",
    "from tensorflow.keras.layers import Dense, Lambda, Dot, Activation, Concatenate\n",
    "from tensorflow.keras.layers import Layer\n",
    "\n",
    "\n",
    "class Attention(Layer):\n",
    "\n",
    "    def __init__(self, units=128, **kwargs):\n",
    "        self.units = units\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    def __call__(self, inputs):\n",
    "        \"\"\"\n",
    "        Many-to-one attention mechanism for Keras.\n",
    "        @param inputs: 3D tensor with shape (batch_size, time_steps, input_dim).\n",
    "        @return: 2D tensor with shape (batch_size, 128)\n",
    "        @author: felixhao28, philipperemy.\n",
    "        \"\"\"\n",
    "        hidden_states = inputs\n",
    "        hidden_size = int(hidden_states.shape[2])\n",
    "        # Inside dense layer\n",
    "        #              hidden_states            dot               W            =>           score_first_part\n",
    "        # (batch_size, time_steps, hidden_size) dot (hidden_size, hidden_size) => (batch_size, time_steps, hidden_size)\n",
    "        # W is the trainable weight matrix of attention Luong's multiplicative style score\n",
    "        score_first_part = Dense(hidden_size, use_bias=False, name='attention_score_vec')(hidden_states)\n",
    "        #            score_first_part           dot        last_hidden_state     => attention_weights\n",
    "        # (batch_size, time_steps, hidden_size) dot   (batch_size, hidden_size)  => (batch_size, time_steps)\n",
    "        h_t = Lambda(lambda x: x[:, -1, :], output_shape=(hidden_size,), name='last_hidden_state')(hidden_states)\n",
    "        score = Dot(axes=[1, 2], name='attention_score')([h_t, score_first_part])\n",
    "        attention_weights = Activation('softmax', name='attention_weight')(score)\n",
    "        # (batch_size, time_steps, hidden_size) dot (batch_size, time_steps) => (batch_size, hidden_size)\n",
    "        context_vector = Dot(axes=[1, 1], name='context_vector')([hidden_states, attention_weights])\n",
    "        pre_activation = Concatenate(name='attention_output')([context_vector, h_t])\n",
    "        attention_vector = Dense(self.units, use_bias=False, activation='tanh', name='attention_vector')(pre_activation)\n",
    "        return attention_vector\n",
    "\n",
    "    def get_config(self):\n",
    "        return {'units': self.units}\n",
    "\n",
    "    @classmethod\n",
    "    def from_config(cls, config):\n",
    "        return cls(**config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 708,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_58\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_58 (Embedding)     (None, 50, 300)           2563500   \n",
      "_________________________________________________________________\n",
      "bidirectional_22 (Bidirectio (None, 50, 128)           186880    \n",
      "_________________________________________________________________\n",
      "layer_11 (Layer)             (None, 50, 128)           0         \n",
      "_________________________________________________________________\n",
      "dense_58 (Dense)             (None, 50, 5)             645       \n",
      "=================================================================\n",
      "Total params: 2,751,025\n",
      "Trainable params: 2,751,025\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "32/32 [==============================] - 6s 111ms/step - loss: 0.2061 - accuracy: 0.9430\n",
      "Epoch 2/10\n",
      "32/32 [==============================] - 4s 111ms/step - loss: 0.0984 - accuracy: 0.9701\n",
      "Epoch 3/10\n",
      "32/32 [==============================] - 4s 113ms/step - loss: 0.0893 - accuracy: 0.9723\n",
      "Epoch 4/10\n",
      "32/32 [==============================] - 4s 120ms/step - loss: 0.0817 - accuracy: 0.9742\n",
      "Epoch 5/10\n",
      "32/32 [==============================] - 4s 113ms/step - loss: 0.0754 - accuracy: 0.9763\n",
      "Epoch 6/10\n",
      "32/32 [==============================] - 4s 114ms/step - loss: 0.0693 - accuracy: 0.9777\n",
      "Epoch 7/10\n",
      "32/32 [==============================] - 4s 113ms/step - loss: 0.0627 - accuracy: 0.9796\n",
      "Epoch 8/10\n",
      "32/32 [==============================] - 4s 114ms/step - loss: 0.0571 - accuracy: 0.9814\n",
      "Epoch 9/10\n",
      "32/32 [==============================] - 4s 112ms/step - loss: 0.0519 - accuracy: 0.9832\n",
      "Epoch 10/10\n",
      "32/32 [==============================] - 4s 113ms/step - loss: 0.0467 - accuracy: 0.9847\n"
     ]
    }
   ],
   "source": [
    "# Attention-based LSTM model\n",
    "import os\n",
    "os.environ['TF_KERAS'] = '1'\n",
    "from keras_self_attention import SeqSelfAttention\n",
    "\n",
    "attentionlstm_model = keras.Sequential()\n",
    "attentionlstm_model.add(tf.keras.layers.Embedding(vocabulary_size, embedding_size, input_length = maxlen))# weights = [embedding_weights], trainable = True))\n",
    "attentionlstm_model.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64, return_sequences=True)))\n",
    "\n",
    "attentionlstm_model.add(Layer(SeqSelfAttention(attention_activation='sigmoid')))\n",
    "\n",
    "attentionlstm_model.add(tf.keras.layers.Dense(5, 'softmax'))\n",
    "\n",
    "attentionlstm_model.summary()\n",
    "attentionlstm_model.compile(optimizer=opt,loss='categorical_crossentropy', metrics=[\"accuracy\"])\n",
    "\n",
    "history = attentionlstm_model.fit(X_train, y_train, batch_size = 128, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 709,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 1s 15ms/step - loss: 0.1308 - accuracy: 0.9675\n",
      "Loss: 0.1308419555425644,\n",
      "Accuracy: 0.9675400257110596\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = attentionlstm_model.evaluate(X_test, y_test, verbose =1)\n",
    "print(\"Loss: {0},\\nAccuracy: {1}\".format(loss, accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 710,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict\n",
    "y_pred_attentionlstm = attentionlstm_model.predict(X_test)\n",
    "\n",
    "# Make hard classes\n",
    "y_pred_attentionlstm[y_pred_attentionlstm > 0.5] = 1\n",
    "y_pred_attentionlstm[y_pred_attentionlstm <= 0.5] = 0\n",
    "\n",
    "y_pred_attentionlstm = np.array(y_pred_attentionlstm.astype('int'))\n",
    "\n",
    "pred_attentionlstm = np.array(tf.argmax(y_pred_attentionlstm, axis = -1))\n",
    "y_true = np.array(tf.argmax(y_test, axis = -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 711,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 48303,   1697],\n",
       "        [  1530, 198470]],\n",
       "\n",
       "       [[198470,   1530],\n",
       "        [  1697,  48303]]])"
      ]
     },
     "execution_count": 711,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Confusion matrix LSTM model - https://scikit-learn.org/stable/modules/generated/sklearn.metrics.multilabel_confusion_matrix.html\n",
    "from sklearn.metrics import multilabel_confusion_matrix\n",
    "multilabel_confusion_matrix(y_test.flatten(), y_pred_attentionlstm.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 712,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.42      0.36      0.39      1150\n",
      "           1       0.40      0.15      0.22       194\n",
      "           2       0.28      0.06      0.10       175\n",
      "           3       0.98      0.99      0.98     48449\n",
      "           4       0.25      0.03      0.06        32\n",
      "\n",
      "    accuracy                           0.97     50000\n",
      "   macro avg       0.46      0.32      0.35     50000\n",
      "weighted avg       0.96      0.97      0.96     50000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Classification report LSTM model\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_true.flatten(), pred_attentionlstm.flatten()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 713,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.96929747\n"
     ]
    }
   ],
   "source": [
    "# Precision LSTM model - https://www.tensorflow.org/api_docs/python/tf/keras/metrics/Precision\n",
    "m = tf.keras.metrics.Precision()\n",
    "m.update_state(y_test, y_pred_attentionlstm)\n",
    "precision = m.result().numpy()\n",
    "print(precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 714,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.96606\n"
     ]
    }
   ],
   "source": [
    "# Recall LSTM model - https://www.tensorflow.org/api_docs/python/tf/keras/metrics/Recall\n",
    "m = tf.keras.metrics.Recall()\n",
    "m.update_state(y_test, y_pred_attentionlstm)\n",
    "recall = m.result().numpy()\n",
    "print(recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 715,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9676760422187805\n"
     ]
    }
   ],
   "source": [
    "# F1-score LSTM model - https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html\n",
    "F1 = 2 * (precision * recall) / (precision + recall)\n",
    "print(F1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
